{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "!pip install transformers\n",
        "from transformers import AutoTokenizer\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfuOZa9FWYki",
        "outputId": "8ce55d87-9066-4fa2-9461-3e0422a66350"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XIaIKPNiR2vh"
      },
      "outputs": [],
      "source": [
        "#transformer model with updating it with mask part\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
        "        super(TransformerLM, self).__init__()\n",
        "        self.mask_token = \"[MASK]\"\n",
        "        self.vocab_size = vocab_size + 1  # Add mask token to vocabulary\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
        "        self.fc = nn.Linear(d_model, self.vocab_size)\n",
        "\n",
        "    def mask_input(self, input_ids):\n",
        "        # Convert input_ids back to tokens for printing\n",
        "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "        print(\"Original Tokens:\", tokens)  # Print the original tokens\n",
        "\n",
        "        num_tokens = len(input_ids)\n",
        "        num_to_mask = int(0.2 * num_tokens)  # Calculate 20% of tokens\n",
        "\n",
        "        masked_indices = random.sample(range(num_tokens), num_to_mask)  # Randomly select indices\n",
        "        labels = [input_ids[i] for i in masked_indices]  # Get true labels for masked tokens\n",
        "\n",
        "        for index in masked_indices:\n",
        "            input_ids[index] = self.vocab_size - 1  # Replace with mask token ID\n",
        "\n",
        "        # Convert masked input_ids back to tokens for printing\n",
        "        masked_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "        print(\"Masked Tokens:\", masked_tokens)  # Print the masked tokens for visualizes\n",
        "\n",
        "        return input_ids, masked_indices, labels\n",
        "\n",
        "    def forward(self, input_ids, masked_indices):\n",
        "        embeds = self.embedding(input_ids)\n",
        "\n",
        "        output = self.transformer(embeds, embeds)\n",
        "        output = self.fc(output)\n",
        "        # Select masked outputs correctly\n",
        "        masked_outputs = output[0, masked_indices, :]\n",
        "        return masked_outputs\n",
        "\n",
        "    def compute_loss(self, masked_outputs, labels):\n",
        "        # Convert labels to tensor and ensure Long type\n",
        "        labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(masked_outputs, labels)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#example\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokens = tokenizer.tokenize(text)\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids_masked, masked_indices, labels = model.mask_input(input_ids.copy())\n",
        "input_ids_masked = torch.tensor([input_ids_masked])\n",
        "\n",
        "# Model and optimizer\n",
        "model = TransformerLM(tokenizer.vocab_size, d_model=512, nhead=8, num_layers=6)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "optimizer.zero_grad()\n",
        "masked_outputs = model(input_ids_masked, masked_indices)\n",
        "loss = model.compute_loss(masked_outputs, labels)\n",
        "\n",
        "# Get predictions and scores (only if there were masked tokens)\n",
        "if loss is not None:\n",
        "    _, predicted_indices = torch.max(masked_outputs, dim=1)\n",
        "    scores = [masked_outputs[i, labels[i]].item() for i in range(len(predicted_indices))]\n",
        "\n",
        "    # Print masked token indices and scores\n",
        "    for index, score in zip(masked_indices, scores):\n",
        "        print(f\"Masked token index: {index}, Score: {score:.4f}\")\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bndwbIAiSNk9",
        "outputId": "171c37f9-f1ea-4c70-a0fd-131826ab0ad9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
            "Masked Tokens: ['the', 'quick', 'brown', 'fox', None, 'over', 'the', 'lazy', None, '.']\n",
            "Masked token index: 8, Score: -0.7705\n",
            "Masked token index: 4, Score: -0.2806\n",
            "Loss: 11.0232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L8GVZsouTAXy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}