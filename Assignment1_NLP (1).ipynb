{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: contrasting different tokenization approaches"
      ],
      "metadata": {
        "id": "0H3EM2cs03Sb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "A39hkMaenBL6"
      },
      "outputs": [],
      "source": [
        "#Import libraries\n",
        "import spacy\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load spaCy model and GPT-2 tokenizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sample texts\n",
        "text_formal=\"Natural language processing is a field of artificial intelligence that focuses on the interaction between computers and humans using natural language.\"\n",
        "text_informal=\"lol 😂 can’t believe how gr8 this new AI model is! #NextLevel 🚀🔥\""
      ],
      "metadata": {
        "id": "g5g5NW0TnsQP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize with spaCy\n",
        "tokens_spacy_formal = [token.text for token in nlp(text_formal)]\n",
        "tokens_spacy_informal = [token.text for token in nlp(text_informal)]\n",
        "\n",
        "print(tokens_spacy_formal)\n",
        "print(tokens_spacy_informal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCBbxWWhpi3T",
        "outputId": "da56d2d8-8a2e-404a-e356-6b89a64b27b8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', '.']\n",
            "['lol', '😂', 'ca', 'n’t', 'believe', 'how', 'gr8', 'this', 'new', 'AI', 'model', 'is', '!', '#', 'NextLevel', '🚀', '🔥']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize with GPT-2\n",
        "tokens_gpt2_formal = tokenizer.tokenize(text_formal)\n",
        "tokens_gpt2_informal = tokenizer.tokenize(text_informal)\n",
        "\n",
        "print(tokens_gpt2_formal)\n",
        "print(tokens_gpt2_informal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh80IYFTqD1e",
        "outputId": "334ce20a-fe9b-461f-825c-73375bbbfd96"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Ġlanguage', 'Ġprocessing', 'Ġis', 'Ġa', 'Ġfield', 'Ġof', 'Ġartificial', 'Ġintelligence', 'Ġthat', 'Ġfocuses', 'Ġon', 'Ġthe', 'Ġinteraction', 'Ġbetween', 'Ġcomputers', 'Ġand', 'Ġhumans', 'Ġusing', 'Ġnatural', 'Ġlanguage', '.']\n",
            "['lol', 'ĠðŁĺ', 'Ĥ', 'Ġcan', 'âĢ', 'Ļ', 't', 'Ġbelieve', 'Ġhow', 'Ġgr', '8', 'Ġthis', 'Ġnew', 'ĠAI', 'Ġmodel', 'Ġis', '!', 'Ġ#', 'Next', 'Level', 'ĠðŁ', 'ļ', 'Ģ', 'ðŁ', 'Ķ', '¥']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observations:\n",
        "## SPACY\n",
        "-handle emojis,hashtags,informal words(gr8)in single token\n",
        "## GPT2\n",
        "-added (G)indicates spaces\n",
        "\n",
        "\n",
        "\n",
        "-Represents emojis and hashtags using Unicode-like encodings\n",
        "\n",
        "Splits informal or rare words (e.g., \"gr8\" into gr and 8)\n"
      ],
      "metadata": {
        "id": "5h0sG7mY1X5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quality of output\n",
        "Formal text: GPT-2 creates subword tokens with prefixes (G indicating space), which may add unnecessary complexity . spaCy provides simpler, human-readable tokens.\n",
        "\n",
        "\n",
        "\n",
        "Informal text: GPT-2 struggles with emojis and creative language, producing fragmented or encoded tokens. spaCy handles these more intuitively, keeping semantic coherence.\n",
        "Both methods have strengths, but spaCy seems better suited for natural language analysis"
      ],
      "metadata": {
        "id": "kgjSvySl2lHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: sentence embeddings\n"
      ],
      "metadata": {
        "id": "yAMKTTlK0xrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import GPT2Model\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "model = GPT2Model.from_pretrained('gpt2')\n",
        "\n"
      ],
      "metadata": {
        "id": "Yp-v0m1Bz9j0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_similarity(sentence1, sentence2):\n",
        "\n",
        "\n",
        "  # Tokenize and get embeddings for each sentence\n",
        "  tokens1 = tokenizer(sentence1, return_tensors='pt')\n",
        "  tokens2 = tokenizer(sentence2, return_tensors='pt')\n",
        "  embeddings1 = model(**tokens1).last_hidden_state.mean(dim=1).detach().numpy()\n",
        "  embeddings2 = model(**tokens2).last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "  # Calculate cosine similarity\n",
        "  similarity = cosine_similarity(embeddings1, embeddings2)[0][0]\n",
        "\n",
        "  return similarity"
      ],
      "metadata": {
        "id": "Pk8OwAGo1PT3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#i will try with bert to see diffrence\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def sentence_similarity_bert(sentence1, sentence2):\n",
        "\n",
        "\n",
        "  # Tokenize and get embeddings for each sentence\n",
        "  inputs1 = tokenizer(sentence1, return_tensors='pt', padding=True, truncation=True)\n",
        "  inputs2 = tokenizer(sentence2, return_tensors='pt', padding=True, truncation=True)\n",
        "  with torch.no_grad():\n",
        "    outputs1 = model(**inputs1).last_hidden_state[:, 0, :].numpy()\n",
        "    outputs2 = model(**inputs2).last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "  #  cosine similarity\n",
        "  similarity = cosine_similarity(outputs1, outputs2)[0][0]\n",
        "\n",
        "  return similarity"
      ],
      "metadata": {
        "id": "oqSJrBeS3yho"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " similar_pairs = [\n",
        "    (\"The cat sat on the mat.\", \"The feline rested on the rug.\"),\n",
        "    (\"She enjoys playing the piano.\", \"She likes to play piano.\"),\n",
        "    (\"He's a skilled programmer.\", \"He's a proficient coder.\")\n",
        "]\n",
        "\n",
        "dissimilar_pairs = [\n",
        "    (\"The weather is sunny today.\", \"I need to buy groceries.\"),\n",
        "    (\"The dog barked loudly.\", \"The coffee is hot.\"),\n",
        "    (\"She's reading a book.\", \"He's playing basketball.\")\n",
        "]\n",
        "\n",
        "# Test with GPT2 similarity function\n",
        "print(\"GPT2 Similarity Scores:\")\n",
        "for pair in similar_pairs:\n",
        "    score = sentence_similarity(pair[0], pair[1])\n",
        "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}': {score}\")\n",
        "\n",
        "for pair in dissimilar_pairs:\n",
        "    score = sentence_similarity(pair[0], pair[1])\n",
        "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}': {score}\")\n",
        "\n",
        "# Test with BERT similarity function\n",
        "print(\"\\nBERT Similarity Scores:\")\n",
        "for pair in similar_pairs:\n",
        "    score = sentence_similarity_bert(pair[0], pair[1])\n",
        "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}': {score}\")\n",
        "\n",
        "for pair in dissimilar_pairs:\n",
        "    score = sentence_similarity_bert(pair[0], pair[1])\n",
        "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}': {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSxLOTTZ4Nby",
        "outputId": "f110e0ce-1683-48d3-ddde-3b8f86bc4156"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2 Similarity Scores:\n",
            "Similarity between 'The cat sat on the mat.' and 'The feline rested on the rug.': 0.8408460021018982\n",
            "Similarity between 'She enjoys playing the piano.' and 'She likes to play piano.': 0.8938599228858948\n",
            "Similarity between 'He's a skilled programmer.' and 'He's a proficient coder.': 0.8806674480438232\n",
            "Similarity between 'The weather is sunny today.' and 'I need to buy groceries.': 0.5917643308639526\n",
            "Similarity between 'The dog barked loudly.' and 'The coffee is hot.': 0.6405841112136841\n",
            "Similarity between 'She's reading a book.' and 'He's playing basketball.': 0.7756593227386475\n",
            "\n",
            "BERT Similarity Scores:\n",
            "Similarity between 'The cat sat on the mat.' and 'The feline rested on the rug.': 0.9503752589225769\n",
            "Similarity between 'She enjoys playing the piano.' and 'She likes to play piano.': 0.9492284655570984\n",
            "Similarity between 'He's a skilled programmer.' and 'He's a proficient coder.': 0.9658842086791992\n",
            "Similarity between 'The weather is sunny today.' and 'I need to buy groceries.': 0.8577530980110168\n",
            "Similarity between 'The dog barked loudly.' and 'The coffee is hot.': 0.8792284727096558\n",
            "Similarity between 'She's reading a book.' and 'He's playing basketball.': 0.9085485339164734\n"
          ]
        }
      ]
    }
  ]
}